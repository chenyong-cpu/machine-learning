{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本嵌入层的代码分析\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn # 预定义的网络层，包含一些现成的工具\n",
    "import math # 数学计算工具包\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# 定义类来实现文本嵌入层\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        '''\n",
    "        d_model:指词嵌入的维度\n",
    "        vocab:指词表的大小\n",
    "        '''\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(10, 3)\n",
    "input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
    "embedding(input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 17.9887, -21.2604,  22.2643,  ..., -17.2853,   6.5057,  -6.7348],\n",
       "          [-21.8817,   8.0818,  14.5089,  ...,  21.4535,   6.0248,  -2.3179],\n",
       "          [ 46.3037,  -3.1423,   1.7513,  ...,  28.7432, -17.5639,  17.1703],\n",
       "          [  8.2219,  17.9838,  17.0450,  ...,  -7.0680,  54.4908,  14.2220]],\n",
       " \n",
       "         [[  3.8480,  13.0536, -35.2835,  ..., -18.1659, -24.6003,   3.8836],\n",
       "          [ 28.0201,  33.8831,  16.9795,  ..., -24.0986,   9.1528,   7.5900],\n",
       "          [-35.8207,   3.6164,  -6.5116,  ...,  17.4589,  26.3022, -20.9856],\n",
       "          [ 13.3537,  28.6510,  32.0331,  ..., -16.0911,  42.2110,  -3.9310]]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " torch.Size([2, 4, 512]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 512\n",
    "vocab = 1000\n",
    "\n",
    "x = Variable(torch.LongTensor([[100, 2, 421, 508], [491, 998, 1, 221]]))\n",
    "emb = Embeddings(d_model, vocab)\n",
    "embr = emb(x)\n",
    "(embr, embr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义编码器类\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 绝对位置矩阵\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2121,  0.8715,  0.9482, -0.8871,  1.0714],\n",
       "        [-0.0000,  0.0000, -0.1685,  0.3339, -1.0596],\n",
       "        [ 1.6276, -0.7780, -1.3569, -0.0000, -0.8909],\n",
       "        [ 0.0000, -0.0000,  1.3403,  0.9246, -0.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Dropout(p=0.2)\n",
    "input = torch.randn(4, 5)\n",
    "output = m(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 3, 4]]),\n",
       " tensor([[1],\n",
       "         [2],\n",
       "         [3],\n",
       "         [4]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4])\n",
    "torch.unsqueeze(x, 0), torch.unsqueeze(x, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('virtualenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb63f12dc7f52f810f0947f49e435da0797b328f197778c8dc581129fa544c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
