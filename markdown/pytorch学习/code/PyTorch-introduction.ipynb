{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "z = torch.zeros(5, 3)\n",
    "print(z)\n",
    "print(z.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "i = torch.ones((5, 3), dtype=torch.int16)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791],\n",
      "        [0.3087, 0.0736]])\n",
      "tensor([[0.4216, 0.0691],\n",
      "        [0.2332, 0.4047]])\n",
      "tensor([[0.3126, 0.3791],\n",
      "        [0.3087, 0.0736]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729)\n",
    "r1 = torch.rand(2, 2)\n",
    "print(r1)\n",
    "\n",
    "r2 = torch.rand(2, 2)\n",
    "print(r2)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "r3 = torch.rand(2, 2)\n",
    "print(r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.ones(2, 3)\n",
    "print(ones)\n",
    "\n",
    "twos = torch.ones(2, 3) * 2\n",
    "print(twos)\n",
    "\n",
    "threes = ones + twos\n",
    "print(threes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1568, -0.8619],\n",
      "        [-0.5336, -0.1907]])\n",
      "tensor([[0.1568, 0.8619],\n",
      "        [0.5336, 0.1907]])\n",
      "tensor([[-0.1574, -1.0390],\n",
      "        [-0.5628, -0.1919]])\n",
      "tensor(-0.4300)\n",
      "torch.return_types.svd(\n",
      "U=tensor([[-0.9135, -0.4069],\n",
      "        [-0.4069,  0.9135]]),\n",
      "S=tensor([0.9370, 0.4589]),\n",
      "V=tensor([[ 0.3846, -0.9231],\n",
      "        [ 0.9231,  0.3846]]))\n",
      "(tensor(0.3312), tensor(-0.4357))\n",
      "tensor(-0.1568)\n"
     ]
    }
   ],
   "source": [
    "r = (torch.rand(2, 2) - 0.5) * 2\n",
    "print(r)\n",
    "\n",
    "print(torch.abs(r))\n",
    "print(torch.asin(r))\n",
    "print(torch.det(r))\n",
    "print(torch.svd(r))\n",
    "print(torch.std_mean(r))\n",
    "print(torch.max(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Image batch shape:\n",
      "torch.Size([1, 1, 32, 32])\n",
      "\n",
      "Raw output:\n",
      "tensor([[ 0.0634,  0.0086, -0.1177, -0.0398,  0.0543, -0.1000,  0.0475,  0.0277,\n",
      "         -0.1264,  0.0808]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "net = LeNet()\n",
    "print(net)                         # what does the object tell us about itself?\n",
    "\n",
    "input = torch.rand(1, 1, 32, 32)   # stand-in for a 32x32 black & white image\n",
    "print('\\nImage batch shape:')\n",
    "print(input.shape)\n",
    "\n",
    "output = net(input)                # we don't call forward() directly\n",
    "print('\\nRaw output:')\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/mapple/1CBAC92C060AAFDC/visual studio code/研究生/machine-learning/markdown/virtualenv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /run/media/mapple/1CBAC92C060AAFDC/visual studio code/研究生/machine-learning/markdown/virtualenv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowEllb\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No such operator profiler::_record_function_enter_new",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/run/media/mapple/1CBAC92C060AAFDC/visual studio code/研究生/machine-learning/markdown/pytorch学习/code/PyTorch-introduction.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/run/media/mapple/1CBAC92C060AAFDC/visual%20studio%20code/%E7%A0%94%E7%A9%B6%E7%94%9F/machine-learning/markdown/pytorch%E5%AD%A6%E4%B9%A0/code/PyTorch-introduction.ipynb#ch0000008?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/run/media/mapple/1CBAC92C060AAFDC/visual%20studio%20code/%E7%A0%94%E7%A9%B6%E7%94%9F/machine-learning/markdown/pytorch%E5%AD%A6%E4%B9%A0/code/PyTorch-introduction.ipynb#ch0000008?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/run/media/mapple/1CBAC92C060AAFDC/visual%20studio%20code/%E7%A0%94%E7%A9%B6%E7%94%9F/machine-learning/markdown/pytorch%E5%AD%A6%E4%B9%A0/code/PyTorch-introduction.ipynb#ch0000008?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtransforms\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/run/media/mapple/1CBAC92C060AAFDC/visual%20studio%20code/%E7%A0%94%E7%A9%B6%E7%94%9F/machine-learning/markdown/pytorch%E5%AD%A6%E4%B9%A0/code/PyTorch-introduction.ipynb#ch0000008?line=4'>5</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose(\n\u001b[1;32m      <a href='vscode-notebook-cell:/run/media/mapple/1CBAC92C060AAFDC/visual%20studio%20code/%E7%A0%94%E7%A9%B6%E7%94%9F/machine-learning/markdown/pytorch%E5%AD%A6%E4%B9%A0/code/PyTorch-introduction.ipynb#ch0000008?line=5'>6</a>\u001b[0m     [transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[1;32m      <a href='vscode-notebook-cell:/run/media/mapple/1CBAC92C060AAFDC/visual%20studio%20code/%E7%A0%94%E7%A9%B6%E7%94%9F/machine-learning/markdown/pytorch%E5%AD%A6%E4%B9%A0/code/PyTorch-introduction.ipynb#ch0000008?line=6'>7</a>\u001b[0m      transforms\u001b[39m.\u001b[39mNormalize((\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m), (\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m))])\n",
      "File \u001b[0;32m/run/media/mapple/1CBAC92C060AAFDC/visual studio code/研究生/machine-learning/markdown/virtualenv/lib/python3.10/site-packages/torchvision/__init__.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m io\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n",
      "File \u001b[0;32m/run/media/mapple/1CBAC92C060AAFDC/visual studio code/研究生/machine-learning/markdown/virtualenv/lib/python3.10/site-packages/torchvision/models/__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39malexnet\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconvnext\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdensenet\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mefficientnet\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m/run/media/mapple/1CBAC92C060AAFDC/visual studio code/研究生/machine-learning/markdown/virtualenv/lib/python3.10/site-packages/torchvision/models/convnext.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn, Tensor\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m functional \u001b[39mas\u001b[39;00m F\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmisc\u001b[39;00m \u001b[39mimport\u001b[39;00m Conv2dNormActivation, Permute\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstochastic_depth\u001b[39;00m \u001b[39mimport\u001b[39;00m StochasticDepth\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_presets\u001b[39;00m \u001b[39mimport\u001b[39;00m ImageClassification\n",
      "File \u001b[0;32m/run/media/mapple/1CBAC92C060AAFDC/visual studio code/研究生/machine-learning/markdown/virtualenv/lib/python3.10/site-packages/torchvision/ops/__init__.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdeform_conv\u001b[39;00m \u001b[39mimport\u001b[39;00m deform_conv2d, DeformConv2d\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdiou_loss\u001b[39;00m \u001b[39mimport\u001b[39;00m distance_box_iou_loss\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdrop_block\u001b[39;00m \u001b[39mimport\u001b[39;00m drop_block2d, DropBlock2d, drop_block3d, DropBlock3d\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfeature_pyramid_network\u001b[39;00m \u001b[39mimport\u001b[39;00m FeaturePyramidNetwork\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfocal_loss\u001b[39;00m \u001b[39mimport\u001b[39;00m sigmoid_focal_loss\n",
      "File \u001b[0;32m/run/media/mapple/1CBAC92C060AAFDC/visual studio code/研究生/machine-learning/markdown/virtualenv/lib/python3.10/site-packages/torchvision/ops/drop_block.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfx\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn, Tensor\n",
      "File \u001b[0;32m/run/media/mapple/1CBAC92C060AAFDC/visual studio code/研究生/machine-learning/markdown/virtualenv/lib/python3.10/site-packages/torch/fx/__init__.py:83\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mr\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mFX is a toolkit for developers to use to transform ``nn.Module``\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39minstances. FX consists of three main components: a **symbolic tracer,**\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mrepository.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgraph_module\u001b[39;00m \u001b[39mimport\u001b[39;00m GraphModule\n\u001b[1;32m     84\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_symbolic_trace\u001b[39;00m \u001b[39mimport\u001b[39;00m symbolic_trace, Tracer, wrap, PH, ProxyableClassMeta\n\u001b[1;32m     85\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgraph\u001b[39;00m \u001b[39mimport\u001b[39;00m Graph, CodeGen\n",
      "File \u001b[0;32m/run/media/mapple/1CBAC92C060AAFDC/visual studio code/研究生/machine-learning/markdown/virtualenv/lib/python3.10/site-packages/torch/fx/graph_module.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlinecache\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Type, Dict, List, Any, Union, Optional, Set\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgraph\u001b[39;00m \u001b[39mimport\u001b[39;00m Graph, _PyTreeCodeGen, _is_from_torch, _custom_builtins, PythonCode\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_compatibility\u001b[39;00m \u001b[39mimport\u001b[39;00m compatibility\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpackage\u001b[39;00m \u001b[39mimport\u001b[39;00m Importer, sys_importer\n",
      "File \u001b[0;32m/run/media/mapple/1CBAC92C060AAFDC/visual studio code/研究生/machine-learning/markdown/virtualenv/lib/python3.10/site-packages/torch/fx/graph.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mnode\u001b[39;00m \u001b[39mimport\u001b[39;00m Node, Argument, Target, map_arg, _type_repr, _get_qualified_name\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_pytree\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpytree\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _pytree \u001b[39mas\u001b[39;00m fx_pytree\n",
      "File \u001b[0;32m/run/media/mapple/1CBAC92C060AAFDC/visual studio code/研究生/machine-learning/markdown/virtualenv/lib/python3.10/site-packages/torch/fx/node.py:31\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m Target \u001b[39m=\u001b[39m Union[Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, Any], \u001b[39mstr\u001b[39m]\n\u001b[1;32m     19\u001b[0m Argument \u001b[39m=\u001b[39m Optional[Union[\n\u001b[1;32m     20\u001b[0m     Tuple[Any, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m],  \u001b[39m# actually Argument, but mypy can't represent recursive types\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     List[Any],  \u001b[39m# actually Argument\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     BaseArgumentTypes\n\u001b[1;32m     26\u001b[0m ]]\n\u001b[1;32m     28\u001b[0m _side_effectful_functions: Set[Callable] \u001b[39m=\u001b[39m {\n\u001b[1;32m     29\u001b[0m     torch\u001b[39m.\u001b[39m_assert,\n\u001b[1;32m     30\u001b[0m     torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39m_record_function_enter,\n\u001b[0;32m---> 31\u001b[0m     torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m_record_function_enter_new,\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39m_record_function_exit}\n\u001b[1;32m     34\u001b[0m \u001b[39m# this is fixed on master, WAR for 1.5\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_find_module_of_method\u001b[39m(orig_method: Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n",
      "File \u001b[0;32m/run/media/mapple/1CBAC92C060AAFDC/visual studio code/研究生/machine-learning/markdown/virtualenv/lib/python3.10/site-packages/torch/_ops.py:167\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39m_OpNamespace\u001b[39;00m(types\u001b[39m.\u001b[39mModuleType):\n\u001b[1;32m    165\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39m    An op namespace to dynamically bind Operators into Python.\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m \n\u001b[1;32m    168\u001b[0m \u001b[39m    Say a user has created a custom Operator called \"my_namespace::my_op\". To\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m    call this op, the user will write torch.ops.my_namespace.my_op(...).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m    At startup, this operation will not yet be bound into Python. Instead, the\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m    following sequence of magic tricks will occur:\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m    1. `torch.ops.my_namespace` will invoke the `__getattr__` magic method\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[39m       on the `torch.ops` object, which will create a new `_OpNamespace`\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39m       object called `my_namespace` and set it as an attribute on the `ops`\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m       object.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39m    2. `torch.ops.my_namespace.my_op` will then invoke `__getattr__` on\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[39m       the `my_namespace` object, which will retrieve the operation via\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39m       `torch.get_operation`, a function bound from C++, and then in a similar\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[39m       fashion bind this new object onto the `my_namespace` object.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[39m    3. `torch.ops.my_namespace.my_op(...)` then calls this new operation\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[39m        and subsequent accesses will incur no further lookup (the namespace and\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39m        operation will already exist).\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[1;32m    185\u001b[0m         \u001b[39msuper\u001b[39m(_OpNamespace, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtorch.ops.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m name)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No such operator profiler::_record_function_enter_new"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('virtualenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb63f12dc7f52f810f0947f49e435da0797b328f197778c8dc581129fa544c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
