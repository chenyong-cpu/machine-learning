{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context='talk')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None): \n",
    "  \"Compute 'Scaled Dot Product Attention'\" \n",
    "  d_k = query.size(-1)\n",
    "  scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "  if mask is not None: \n",
    "    scores = scores.masked_fill(mask == 0, -1e9) \n",
    "  p_attn = F.softmax(scores, dim = -1) \n",
    "  if dropout is not None: \n",
    "    p_attn = dropout(p_attn)\n",
    "  return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module): \n",
    "  def __init__(self, h, d_model, dropout=0.1): \n",
    "    \"Take in model size and number of heads.\" \n",
    "    super(MultiHeadedAttention, self).__init__() \n",
    "    assert d_model % h == 0\n",
    "    self.d_k = d_model // h\n",
    "    self.h = h\n",
    "    self.linears = clones(nn.Linear(d_model, d_model), 4) \n",
    "\n",
    "    self.attn = None \n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "  def forward(self, query, key, value, mask=None): \n",
    "    if mask is not None: \n",
    "      mask = mask.unsqueeze(1)\n",
    "    nbatches = query.size(0)\n",
    "    query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k)\n",
    "      .transpose(1, 2) for l, x in \n",
    "      zip(self.linears, (query, key, value))] \n",
    "    x, self.attn = attention(query, key, value, mask=mask, \n",
    "      dropout=self.dropout) \n",
    "    x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k) \n",
    "\n",
    "    return self.linears[-1](x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        # features=d_model=512, eps=epsilon 用于分母的非0化平滑\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        # a_2 是一个可训练参数向量，(512)\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        # b_2 也是一个可训练参数向量, (512)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 的形状为(batch.size, sequence.len, 512)\n",
    "        mean = x.mean(-1, keepdim=True) \n",
    "        # 对x的最后一个维度，取平均值，得到tensor (batch.size, seq.len)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        # 对x的最后一个维度，取标准方差，得(batch.size, seq.len)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "        # 本质上类似于（x-mean)/std，不过这里加入了两个可训练向量\n",
    "        # a_2 and b_2，以及分母上增加一个极小值epsilon，用来防止std为0\n",
    "        # 的时候的除法溢出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        # size=d_model=512; dropout=0.1\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size) # (512)，用来定义a_2和b_2\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the \"\n",
    "        \"same size.\"\n",
    "        # x is alike (batch.size, sequence.len, 512)\n",
    "        # sublayer是一个具体的MultiHeadAttention\n",
    "        #或者PositionwiseFeedForward对象\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "        # x (30, 10, 512) -> norm (LayerNorm) -> (30, 10, 512)\n",
    "        # -> sublayer (MultiHeadAttention or PositionwiseFeedForward)\n",
    "        # -> (30, 10, 512) -> dropout -> (30, 10, 512)\n",
    "        \n",
    "        # 然后输入的x（没有走sublayer) + 上面的结果，\n",
    "        #即实现了残差相加的功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        # d_model = 512\n",
    "        # d_ff = 2048 = 512*4\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        # 构建第一个全连接层，(512, 2048)，其中有两种可训练参数：\n",
    "        # weights矩阵，(512, 2048)，以及\n",
    "        # biases偏移向量, (2048)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        # 构建第二个全连接层, (2048, 512)，两种可训练参数：\n",
    "        # weights矩阵，(2048, 512)，以及\n",
    "        # biases偏移向量, (512)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape = (batch.size, sequence.len, 512)\n",
    "        # 例如, (30, 10, 512)\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "        # x (30, 10, 512) -> self.w_1 -> (30, 10, 2048)\n",
    "        # -> relu -> (30, 10, 2048) \n",
    "        # -> dropout -> (30, 10, 2048)\n",
    "        # -> self.w_2 -> (30, 10, 512)是输出的shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and \"\n",
    "    \"feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        # size=d_model=512\n",
    "        # self_attn = MultiHeadAttention对象, first sublayer\n",
    "        # feed_forward = PositionwiseFeedForward对象，second sublayer\n",
    "        # dropout = 0.1 (e.g.)\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        # 使用深度克隆方法，完整地复制出来两个SublayerConnection\n",
    "        self.size = size # 512\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        # x shape = (30, 10, 512)\n",
    "        # mask 是(batch.size, 10,10)的矩阵，类似于当前一个词w，有哪些词是w可见的\n",
    "        # 源语言的序列的话，所有其他词都可见，除了\"<blank>\"这样的填充；\n",
    "        # 目标语言的序列的话，所有w的左边的词，都可见。\n",
    "        x = self.sublayer[0](x, \n",
    "          lambda x: self.self_attn(x, x, x, mask))\n",
    "        # x (30, 10, 512) -> self_attn (MultiHeadAttention) \n",
    "        # shape is same (30, 10, 512) -> SublayerConnection \n",
    "        # -> (30, 10, 512)\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "        # x 和feed_forward对象一起，给第二个SublayerConnection\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        # layer = one EncoderLayer object, N=6\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N) \n",
    "        # 深copy，N=6，\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        # 定义一个LayerNorm，layer.size=d_model=512\n",
    "        # 其中有两个可训练参数a_2和b_2\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        # x is alike (30, 10, 512)\n",
    "        # (batch.size, sequence.len, d_model)\n",
    "        # mask是类似于(batch.size, 10, 10)的矩阵\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "            # 进行六次EncoderLayer操作\n",
    "        return self.norm(x)\n",
    "        # 最后做一次LayerNorm，最后的输出也是(30, 10, 512) shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, \"\n",
    "    \"and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, \n",
    "      feed_forward, dropout):\n",
    "      # size = d_model=512,\n",
    "      # self_attn = one MultiHeadAttention object，目标语言序列的\n",
    "      # src_attn = second MultiHeadAttention object, 目标语言序列\n",
    "      # 和源语言序列之间的\n",
    "      # feed_forward 一个全连接层\n",
    "      # dropout = 0.1\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size # 512\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "        # 需要三个SublayerConnection, 分别在\n",
    "        # self.self_attn, self.src_attn, 和self.feed_forward\n",
    "        # 的后边\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory # (batch.size, sequence.len, 512) \n",
    "        # 来自源语言序列的Encoder之后的输出，作为memory\n",
    "        # 供目标语言的序列检索匹配：（类似于alignment in SMT)\n",
    "        x = self.sublayer[0](x, \n",
    "          lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        # 通过一个匿名函数，来实现目标序列的自注意力编码\n",
    "        # 结果扔给sublayer[0]:SublayerConnection\n",
    "        x = self.sublayer[1](x, \n",
    "          lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        # 通过第二个匿名函数，来实现目标序列和源序列的注意力计算\n",
    "        # 结果扔给sublayer[1]:SublayerConnection\n",
    "        return self.sublayer[2](x, self.feed_forward)\n",
    "        # 走一个全连接层，然后\n",
    "        # 结果扔给sublayer[2]:SublayerConnection\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        # layer = DecoderLayer object\n",
    "        # N = 6\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        # 深度copy六次DecoderLayer\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        # 初始化一个LayerNorm\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "            # 执行六次DecoderLayer\n",
    "        return self.norm(x)\n",
    "        # 执行一次LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        # d_model=512\n",
    "        # vocab = 目标语言词表大小\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "        # 定义一个全连接层，可训练参数个数是(512 * trg_vocab_size) + \n",
    "        # trg_vocab_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n",
    "        # x 类似于 (batch.size, sequence.length, 512)\n",
    "        # -> proj 全连接层 (30, 10, trg_vocab_size) = logits\n",
    "        # 对最后一个维度执行log_soft_max\n",
    "        # 得到(30, 10, trg_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. \n",
    "    Base for this and many other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, \n",
    "      src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        # Encoder对象\n",
    "        self.decoder = decoder\n",
    "        # Decoder对象\n",
    "        self.src_embed = src_embed\n",
    "        # 源语言序列的编码，包括词嵌入和位置编码\n",
    "        self.tgt_embed = tgt_embed\n",
    "        # 目标语言序列的编码，包括词嵌入和位置编码\n",
    "        self.generator = generator\n",
    "        # 生成器\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "        # 先对源语言序列进行编码，\n",
    "        # 结果作为memory传递给目标语言的编码器\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # src = (batch.size, seq.length)\n",
    "        # src_mask 负责对src加掩码\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "        # 对源语言序列进行编码，得到的结果为\n",
    "        # (batch.size, seq.length, 512)的tensor\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), \n",
    "          memory, src_mask, tgt_mask)\n",
    "        # 对目标语言序列进行编码，得到的结果为\n",
    "        # (batch.size, seq.length, 512)的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    # e.g., size=10\n",
    "    attn_shape = (1, size, size) # (1, 10, 10)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    # triu: 负责生成一个三角矩阵，k-th对角线以下都是设置为0 \n",
    "    # 上三角中元素为1.\n",
    "    \n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "    # 反转上面的triu得到的上三角矩阵，修改为下三角矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    # src_vocab = 源语言词表大小\n",
    "    # tgt_vocab = 目标语言词表大小\n",
    "    \n",
    "    c = copy.deepcopy # 对象的深度copy/clone\n",
    "    attn = MultiHeadedAttention(h, d_model) # 8, 512\n",
    "    # 构造一个MultiHeadAttention对象\n",
    "    \n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    # 512, 2048, 0.1\n",
    "    # 构造一个feed forward对象\n",
    "\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    # 位置编码\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "\n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model # EncoderDecoder 对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = make_model(30000, 30000, 6) \n",
    "# src_vocab_size=30000, tgt_vocab_size=30000, N=6\n",
    "#None\n",
    "for name, param in tmp_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data.shape)\n",
    "    else:\n",
    "        print ('no gradient necessary', name, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        # src: 源语言序列，(batch.size, src.seq.len)\n",
    "        # 二维tensor，第一维度是batch.size；第二个维度是源语言句子的长度\n",
    "        # 例如：[ [2,1,3,4], [2,3,1,4] ]这样的二行四列的，\n",
    "        # 1-4代表每个单词word的id\n",
    "        \n",
    "        # trg: 目标语言序列，默认为空，其shape和src类似\n",
    "        # (batch.size, trg.seq.len)，\n",
    "        #二维tensor，第一维度是batch.size；第二个维度是目标语言句子的长度\n",
    "        # 例如trg=[ [2,1,3,4], [2,3,1,4] ] for a \"copy network\"\n",
    "        # (输出序列和输入序列完全相同）\n",
    "        \n",
    "        # pad: 源语言和目标语言统一使用的 位置填充符号，'<blank>'\n",
    "        # 所对应的id，这里默认为0\n",
    "        # 例如，如果一个source sequence，长度不到4，则在右边补0\n",
    "        # [1,2] -> [1,2,0,0]\n",
    "        \n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        # src = (batch.size, seq.len) -> != pad -> \n",
    "        # (batch.size, seq.len) -> usnqueeze ->\n",
    "        # (batch.size, 1, seq.len) 相当于在倒数第二个维度扩展\n",
    "        # e.g., src=[ [2,1,3,4], [2,3,1,4] ]对应的是\n",
    "        # src_mask=[ [[1,1,1,1], [1,1,1,1]] ]\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1] # 重要\n",
    "            # trg 相当于目标序列的前N-1个单词的序列\n",
    "            #（去掉了最后一个词）\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            # trg_y 相当于目标序列的后N-1个单词的序列\n",
    "            # (去掉了第一个词）\n",
    "            # 目的是(src + trg) 来预测出来(trg_y)，\n",
    "            # 这个在illustrated transformer中详细图示过。\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        # 这里的tgt类似于：\n",
    "        #[ [2,1,3], [2,3,1] ] （最初的输入目标序列，分别去掉了最后一个词\n",
    "        # pad=0, '<blank>'的id编号\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        # 得到的tgt_mask类似于\n",
    "        # tgt_mask = tensor([[[1, 1, 1]],[[1, 1, 1]]], dtype=torch.uint8)\n",
    "        # shape=(2,1,3)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        # 先看subsequent_mask, 其输入的是tgt.size(-1)=3\n",
    "        # 这个函数的输出为= tensor([[[1, 0, 0],\n",
    "        # [1, 1, 0],\n",
    "        # [1, 1, 1]]], dtype=torch.uint8)\n",
    "        # type_as 把这个tensor转成tgt_mask.data的type(也是torch.uint8)\n",
    "        \n",
    "        # 这样的话，&的两边的tensor分别是(2,1,3), (1,3,3);\n",
    "        #tgt_mask = tensor([[[1, 1, 1]],[[1, 1, 1]]], dtype=torch.uint8)\n",
    "        #and\n",
    "        # tensor([[[1, 0, 0], [1, 1, 0], [1, 1, 1]]], dtype=torch.uint8)\n",
    "        \n",
    "        # (2,3,3)就是得到的tensor\n",
    "        # tgt_mask.data = tensor([[[1, 0, 0],\n",
    "        # [1, 1, 0],\n",
    "        # [1, 1, 1]],\n",
    "\n",
    "        #[[1, 0, 0],\n",
    "        # [1, 1, 0],\n",
    "        # [1, 1, 1]]], dtype=torch.uint8)\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # optimizer = Adam (Parameter Group 0\n",
    "        #    amsgrad: False\n",
    "        #    betas: (0.9, 0.98)\n",
    "        #    eps: 1e-09\n",
    "        #    lr: 0\n",
    "        #    weight_decay: 0\n",
    "        #)\n",
    "        self._step = 0\n",
    "        self.warmup = warmup # e.g., 4000 轮 热身\n",
    "        self.factor = factor # e.g., 2\n",
    "        self.model_size = model_size # 512\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate`(learning rate) above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, \n",
    "            betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx # '<blank>' 的id\n",
    "        self.confidence = 1.0 - smoothing # 自留的概率值、得分 e.g. 0.6\n",
    "        self.smoothing = smoothing # 均分出去的概率值，得分 e.g. 0.4\n",
    "        self.size = size # target vocab size 目标语言词表大小\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        \"in real-world case: 真实情况下\"\n",
    "        #  x的shape为(batch.size * seq.len, target.vocab.size)\n",
    "        # y的shape是(batch.size * seq.len)\n",
    "        \n",
    "        # x=logits，(seq.len, target.vocab.size)\n",
    "        # 每一行，代表一个位置的词\n",
    "        # 类似于：假设seq.len=3, target.vocab.size=5\n",
    "        # x中保存的是log(prob)\n",
    "        #x = tensor([[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "        #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "        #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233]])\n",
    "        \n",
    "        # target 类似于：\n",
    "        # target = tensor([2, 1, 0])，torch.size=(3)\n",
    "        \n",
    "        assert x.size(1) == self.size # 目标语言词表大小\n",
    "        true_dist = x.data.clone()\n",
    "        # true_dist = tensor([[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "        #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "        #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233]])\n",
    "        \n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        # true_dist = tensor([[0.1333, 0.1333, 0.1333, 0.1333, 0.1333],\n",
    "        #[0.1333, 0.1333, 0.1333, 0.1333, 0.1333],\n",
    "        #[0.1333, 0.1333, 0.1333, 0.1333, 0.1333]])\n",
    "        \n",
    "        # 注意，这里分母target.vocab.size-2是因为\n",
    "        # (1) 最优值 0.6要占一个位置；\n",
    "        # (2) 填充词 <blank> 要被排除在外\n",
    "        # 所以被激活的目标语言词表大小就是self.size-2\n",
    "        \n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), \n",
    "          self.confidence)\n",
    "        # target.data.unsqueeze(1) -> \n",
    "        # tensor([[2],\n",
    "        #[1],\n",
    "        #[0]]); shape=torch.Size([3, 1])  \n",
    "        # self.confidence = 0.6\n",
    "        \n",
    "        # 根据target.data的指示，按照列优先(1)的原则，把0.6这个值\n",
    "        # 填入true_dist: 因为target.data是2,1,0的内容，\n",
    "        # 所以，0.6填入第0行的第2列（列号，行号都是0开始）\n",
    "        # 0.6填入第1行的第1列\n",
    "        # 0.6填入第2行的第0列：\n",
    "        # true_dist = tensor([[0.1333, 0.1333, 0.6000, 0.1333, 0.1333],\n",
    "        #[0.1333, 0.6000, 0.1333, 0.1333, 0.1333],\n",
    "        #[0.6000, 0.1333, 0.1333, 0.1333, 0.1333]])\n",
    "          \n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        # true_dist = tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
    "        #[0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n",
    "        #[0.0000, 0.1333, 0.1333, 0.1333, 0.1333]])\n",
    "        # 设置true_dist这个tensor的第一列的值全为0\n",
    "        # 因为这个是填充词'<blank>'所在的id位置，不应该计入\n",
    "        # 目标词表。需要注意的是，true_dist的每一列，代表目标语言词表\n",
    "        #中的一个词的id\n",
    "        \n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        # mask = tensor([[2]]), 也就是说，最后一个词 2,1,0中的0，\n",
    "        # 因为是'<blank>'的id，所以通过上面的一步，把他们找出来\n",
    "        \n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "            # 当target reference序列中有0这个'<blank>'的时候，则需要把\n",
    "            # 这一行的值都清空。\n",
    "            # 在一个batch里面的时候，可能两个序列长度不一，所以短的序列需要\n",
    "            # pad '<blank>'来填充，所以会出现类似于(2,1,0)这样的情况\n",
    "            # true_dist = tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
    "            # [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n",
    "            # [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, \n",
    "          Variable(true_dist, requires_grad=False))\n",
    "          # 这一步就是调用KL loss来计算\n",
    "          # x = tensor([[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "          #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "          #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233]])\n",
    "          \n",
    "          # true_dist=tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
    "          # [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n",
    "          # [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
    "          # 之间的loss了。细节可以参考我的那篇illustrated transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V, batch, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
    "        data[:, 0] = 1\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator # Generator 对象, linear+softmax\n",
    "        self.criterion = criterion # LabelSmooth对象，计算loss\n",
    "        self.opt = opt # NormOpt对象，优化算法对象\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        # e.g., x为(2,3,8), batch.size=2, seq.len=3, d_model=8\n",
    "        # y = tensor([[4, 2, 1],\n",
    "        #[4, 4, 4]], dtype=torch.int32)\n",
    "        \n",
    "        # norm: (y=trg_y中非'<blank>'的token的个数)\n",
    "        \"attention here\"\n",
    "        \n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
    "                              y.contiguous().view(-1)) / norm.item()\n",
    "        # 变形后，x类似于(batch.size*seq.len, target.vocab.size)\n",
    "        # y为(target.vocab.size)\n",
    "        # 然后调用LabelSmooth来计算loss\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        #return loss.data[0] * norm\n",
    "        \"attention here\"\n",
    "        return loss.data.item() * norm.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3752170130.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [27]\u001b[1;36m\u001b[0m\n\u001b[1;33m    ef run_epoch(aepoch, data_iter, model, loss_compute):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "ef run_epoch(aepoch, data_iter, model, loss_compute):\n",
    "\n",
    "    \"Standard Training and Logging Function\"\n",
    "    # data_iter = 所有数据的打包\n",
    "    # model = EncoderDecoder 对象\n",
    "    # loss_compute = SimpleLossCompute对象\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        # 对每个batch循环\n",
    "        out = model.forward(batch.src, batch.trg, \n",
    "                            batch.src_mask, batch.trg_mask)\n",
    "        # 使用目前的model，对batch.src+batch.trg进行forward\n",
    "                            \n",
    "        # e.g.,\n",
    "        # batch.src (2,4) = tensor([[1, 4, 2, 1],\n",
    "        # [1, 4, 4, 4]], dtype=torch.int32)\n",
    "        \n",
    "        # batch.trg (2,3) = tensor([[1, 4, 2],\n",
    "        # [1, 4, 4]], dtype=torch.int32)\n",
    "        \n",
    "        # batch.src_mask (2,1,4) = tensor([[[1, 1, 1, 1]],\n",
    "        # [[1, 1, 1, 1]]], dtype=torch.uint8)\n",
    "        \n",
    "        # batch.trg_mask (2,3,3) = tensor([[[1, 0, 0],\n",
    "        # [1, 1, 0],\n",
    "        # [1, 1, 1]],\n",
    "\n",
    "        #[[1, 0, 0],\n",
    "        # [1, 1, 0],\n",
    "         #[1, 1, 1]]], dtype=torch.uint8)\n",
    "         \n",
    "        # and out (2,3,8):\n",
    "        # out = tensor([[[-0.4749, -0.4887,  0.1245, -0.4042,  0.5301,  \n",
    "        #   1.7662, -1.6224, 0.5694],\n",
    "        # [ 0.4683, -0.7813,  0.2845,  0.4464, -0.3088, -0.1751, -1.6643,\n",
    "        #   1.7303],\n",
    "         #[-1.1600, -0.2348,  1.0631,  1.3192, -0.9453,  0.3538,  0.7051...                 \n",
    "        \n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        # out和trg_y计算Loss\n",
    "        # ntokens = 6 (trg_y中非'<blank>'的token的个数)\n",
    "        # 注意，这里是token,不是unique word\n",
    "        # 例如[ [ [1, 2, 3], [2,3,4] ]中有6个token,而只有4个unique word\n",
    "        \n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            \"attention here 这里隐藏一个bug\"\n",
    "            #print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "            #        (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            print ('epoch step: {}:{} Loss: {}/{}, tokens per sec: {}/{}'\n",
    "                    .format(aepoch, i, loss, batch.ntokens, \n",
    "                    tokens, elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('jupyter': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "141c5e0ba87e97013844de5b21d12d023199bf67a08052ba51b9f2de1a481a39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
