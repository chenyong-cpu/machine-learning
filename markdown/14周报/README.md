# ç ”ç©¶ç”Ÿå‘¨æŠ¥ï¼ˆç¬¬åä¸ƒå‘¨ï¼‰

## å­¦ä¹ ç›®æ ‡

1. åŸºæœ¬çœ‹å®Œè¯­éŸ³ä¸è¯­è¨€å¤„ç†ä¸€ä¹¦
2. çœ‹Transformerçš„ç»“æ„ä¸å®ç°ä»£ç 

## å­¦ä¹ æ—¶é—´

> 8.28 ~ 9.03

## å­¦ä¹ äº§å‡º

### æ³¨æ„åŠ›æœºåˆ¶å’ŒTransformer

1. Vector Set as Inputï¼ˆè¾“å…¥æ˜¯ä¸€å †å‘é‡ï¼‰
   1. One-hot Encoding å„ä¸ªè¯ä¹‹é—´æ²¡æœ‰å…³ç³»ï¼Œå¹¶ä¸”æœ€åçš„è¾“å…¥åºåˆ—ä¼šå¾ˆå¤§
   2. Word Embedding ç»™æ¯ä¸€ä¸ªè¯æ±‡ä¸€ä¸ªå‘é‡ï¼Œä¸€ç±»è¯æ±‡å¯èƒ½å…±äº«ä¸€ä¸ªå‘é‡
2. è¾“å‡º
   1. each vector has a labelï¼šè¯æ€§æ ‡æ³¨ç­‰
   2. The whole sequence has a labelï¼šä¸€æ®µè¯­éŸ³çš„è®²è¯è€…ç­‰
   3. Model decides the number of labels itselfï¼šseq2seqï¼ˆç¿»è¯‘ç­‰ï¼‰

![avator](./image/1.png)

```python
query, key, value = \
            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
             for l, x in zip(self.linears, (query, key, value))]
```

å…¶ä¸­$W^qã€W^kã€W^v$æ˜¯æœªçŸ¥å‚æ•°ï¼Œç›¸å½“äºå…¨è¿æ¥å±‚ï¼Œç›´æ¥**ç‚¼ä¸¹**å‡ºæ¥qã€kã€vï¼Ÿ

$$
\begin{aligned}
q^i=W^qa^i; \
q^k=W^ka^i; \
q^v=W^va^i \
\end{aligned}
$$

![avator](./image/2.png)

![avator](./image/3.png)

å…¶ä¸­softmaxä¹Ÿå¯ä»¥ç”¨reluç­‰æ›¿æ¢ï¼Œæ•ˆæœå¯èƒ½å·®ä¸€ç‚¹ã€‚

![avator](./image/4.png)

**Multi-head Self-attention**

ç”¨qæ‰¾ä¸åŒçš„ç›¸å…³ï¼Œç›¸å…³æ€§å¯èƒ½å¤šç§å¤šæ ·ï¼Œå› æ­¤qåº”è¯¥è¦å¤šä¸ªï¼Œkã€vä¹Ÿè¦å¤šä¸ªã€‚

![avator](./image/5.png)

å› æ­¤ä¼šäº§ç”Ÿ$b^{i,1}ã€b^{i,2}$ç­‰å¤šä¸ªå‚æ•°ï¼Œéœ€è¦è¿›è¡Œå¦‚ä¸‹å¤„ç†ï¼š

$$
b^i = W^o
\left[
\begin{matrix}
    b^{i,1} \\
    b^{i,2}
\end{matrix}
\right]
$$

$W^o$åº”è¯¥ä¹Ÿæ˜¯è¶…å‚æ•°ï¼Œéœ€è¦æ¨¡å‹è‡ªå·±è°ƒæ•´ã€‚

**Positional Encoding**

å‰é¢çš„è¿‡ç¨‹ä¸­æ¯ä¸ªå‘é‡éƒ½æ˜¯å¹³ç­‰çš„ï¼Œå¯ä»¥äº’ç›¸æ¢ä½ç½®è€Œå¯¹ç»“æœæ²¡æœ‰å½±å“ï¼Œè¯­è¨€æ˜¾ç„¶ä¸æ˜¯è¿™æ ·çš„ã€‚
é€šè¿‡å¯¹æ¯ä¸ªå‘é‡æ·»åŠ $e^i$å‘é‡è¿›è¡Œè§£å†³ã€‚

$$
e^i + a^i
$$

æ¯”å¦‚æ·»åŠ å¦‚ä¸‹çš„ä½ç½®å‘é‡ï¼Œä¸‹é¢è¿™ä¸ªæ˜¯äººä¸ºè®¾ç½®ï¼ˆç‚¼ä¸¹ï¼‰çš„ã€‚

![avator](./image/6.png)

ç›®å‰æœ‰å„ç§æ–¹æ³•äº§ç”ŸPositional Encodingã€‚

**Self-attentionå’ŒCNNçš„æ¯”è¾ƒ**

CNNå¯ä»¥çœ‹ä½œä¸€ç§ç®€åŒ–ç‰ˆçš„Self-attentionã€‚

ä»€ä¹ˆæ ·çš„Self-attentioné€Ÿåº¦å¿«æ•ˆæœåˆå¥½ï¼Œè¿™æ˜¯å€¼å¾—ç‚¼ä¸¹çš„â€¦â€¦

![avator](./image/7.png)

**Masked Self-attention**

æŠŠå½“å‰å¤„ç†å€¼åé¢çš„å†…å®¹å¿½ç•¥ã€‚

**AT vs NAT**

ç”±äºä¸çŸ¥é“è¦è¾“å‡ºå¤šå°‘é•¿åº¦çš„ç»“æœï¼Œå› æ­¤éœ€è¦åˆ¤æ–­å¥å­ä»€ä¹ˆæ—¶å€™ç»“æŸã€‚

![avator](./image/8.png)

NATåˆ¤æ–­ç»“æŸï¼š
   1. ç”¨å¦ä¸€ä¸ªé¢„æµ‹å™¨æ¥åˆ¤æ–­å¥å­çš„é•¿åº¦
   2. æŠŠEndåé¢çš„å…¨éƒ¨å¹²æ‰
NATä¼˜åŠ¿ï¼š
   1. å¹¶è¡ŒåŒ–
   2. å¯æ§çš„è¾“å‡ºé•¿åº¦

ä¸è¿‡é€šå¸¸NATæ¯”ATåğŸ˜Š

BLEU scoreï¼Ÿæ¯”è¾ƒæ¨¡å‹ç”Ÿæˆçš„å¥å­å’ŒåŸå¥å­çš„å·®è·

**beam search**

**BatchNorm vs LayerNorm**

BatchNormçš„è®¡ç®—æµç¨‹

$$
\begin{aligned}
   &\mu_{\beta} \leftarrow \frac{1}{m}\sum_{i=1}^{m}x^i \\
   &\sigma_{\beta}^2 \leftarrow \frac{1}{m}\sum_{i=1}^{m}(x_i-\mu_{\beta})^2 \\
   &\hat{x_i} \leftarrow \frac{x_i-\mu_{beta}}{\sqrt{\sigma^2_{\beta}+\epsilon}} \\
   &y_i \leftarrow \Upsilon \hat{x_i}+\beta
\end{aligned}
$$

- $\epsilon$ï¼šæ·»åŠ è¾ƒå°çš„å€¼åˆ°æ–¹å·®ä¸­æ”¾ç½®é™¤äº0
- $\Upsilon$ï¼šå¯è®­ç»ƒçš„æ¯”ä¾‹å‚æ•°
- $\beta$ï¼šå¯è®­ç»ƒçš„åå·®å‚æ•°

"æ‰¹é‡å½’ä¸€åŒ–"è¿™ä¸ªåè¯æœ‰ç‚¹å¤±è´¥ã€‚å½’ä¸€åŒ–ï¼Œæ„å‘³ç€æŠŠtensorä¸­çš„åª’ä½“ä¸ªå…ƒç´ è§„èŒƒåŒ–åˆ°[0,1]åŒºé—´å†…ã€‚è€Œtanhã€sigmoidä¹Ÿå…·æœ‰è¿™ä¸ªä½œç”¨ã€‚æ‰¹é‡å½’ä¸€åŒ–çœŸæ­£çš„ç²¾å¦™ä¹‹å¤„åœ¨äºç”¨ä¸€ç§æä¸ºèŠ‚èƒ½çš„æ–¹å¼å»ºæ¨¡äº†æ•´ä½“ä¸ä¸ªä½“ä¹‹é—´çš„ç›¸å¯¹å…³ç³»ã€‚

BatchNormçš„æµç¨‹ä¸ºï¼š
1. è®¡ç®—/æ›´æ–°å‡å€¼
2. è®¡ç®—/æ›´æ–°æ–¹å·®
3. ä½¿ç”¨å‡å€¼å’Œæ–¹å·®å°†æ¯ä¸ªå…ƒç´ æ ‡å‡†åŒ–

```python
import numpy as np

def Batchnorm(x, gamma, beta, bn_param):
    # x_shape:[B, C, H, W]
    running_mean = bn_param['running_mean']
    running_var = bn_param['running_var']
    results = 0.
    eps = 1e-5

    x_mean = np.mean(x, axis=(0, 2, 3), keepdims=True)
    x_var = np.var(x, axis=(0, 2, 3), keepdims=True0)
    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)
    results = gamma * x_normalized + beta

    # å› ä¸ºåœ¨æµ‹è¯•æ—¶æ˜¯å•ä¸ªå›¾ç‰‡æµ‹è¯•ï¼Œè¿™é‡Œä¿ç•™è®­ç»ƒæ—¶çš„å‡å€¼å’Œæ–¹å·®ï¼Œç”¨åœ¨åé¢æµ‹è¯•æ—¶ç”¨
    running_mean = momentum * running_mean + (1 - momentum) * x_mean
    running_var = momentum * running_var + (1 - momentum) * x_var

    bn_param['running_mean'] = running_mean
    bn_param['running_var'] = running_var

    return results, bn_param
```

å°batch-sizeä¸­ä½¿ç”¨BatchNormä¼šç ´åæ€§èƒ½ã€‚

BatchNormçš„è®¡ç®—å…¬å¼

$$
\begin{aligned}
   &\mu^{l}=\frac{1}{H}\sum_{i=1}^Ha_i^l \\
   &\sigma^l=\sqrt{\frac{1}{H}\sum_{i=1}^H(a_i^l-\mu^l)^2}
\end{aligned}
$$

```python
def Layernorm(x, gamma, beta):

    results = 0.
    eps = 1e-5

    x_mean = np.mean(x, axis=(1, 2, 3), keepdims=True)
    x_var = np.var(x, axis=(1, 2, 3), keepdims=True0)
    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)
    results = gamma * x_normalized + beta
    return results
```

## æ€»ç»“

1. ã€Šè¯­éŸ³ä¸è¯­éŸ³å¤„ç†ã€‹ä¸€ä¹¦ä¸­åé¢çš„å†…å®¹å¤§å¤šå…³äºå¥å­ç»“æ„ä¸å¥å­å«ä¹‰ï¼Œå¤§è‡´ä¸Šå¿«é€Ÿåœ°çœ‹äº†ä»¥ä¸‹ã€‚
2. Transformersçš„ç»“æ„å¤§è‡´äº†è§£çš„å·®ä¸å¤šï¼Œç°åœ¨è¿˜åœ¨çœ‹æ•´ä¸ªTransformersçš„ä»£ç ï¼Œé¢„è®¡ä¸‹å‘¨å»è·‘ä¸€ä¸‹çœ‹çœ‹æ•ˆæœã€‚
3. é¡¹ç›®ä¸Šçš„è¯ä¸»è¦æ˜¯åœ¨æé«˜åœ°å€åˆ’åˆ†çš„ç²¾å‡†åº¦ã€‚
