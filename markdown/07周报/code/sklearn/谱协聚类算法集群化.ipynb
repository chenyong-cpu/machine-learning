{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\visual studio code\\研究生\\machine-learning\\markdown\\07周报\\code\\sklearn\\谱协聚类算法集群化.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/visual%20studio%20code/%E7%A0%94%E7%A9%B6%E7%94%9F/machine-learning/markdown/07%E5%91%A8%E6%8A%A5/code/sklearn/%E8%B0%B1%E5%8D%8F%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E9%9B%86%E7%BE%A4%E5%8C%96.ipynb#ch0000000?line=31'>32</a>\u001b[0m \u001b[39m# exclude 'comp.os.ms-windows.misc'\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/visual%20studio%20code/%E7%A0%94%E7%A9%B6%E7%94%9F/machine-learning/markdown/07%E5%91%A8%E6%8A%A5/code/sklearn/%E8%B0%B1%E5%8D%8F%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E9%9B%86%E7%BE%A4%E5%8C%96.ipynb#ch0000000?line=32'>33</a>\u001b[0m categories \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39malt.atheism\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcomp.graphics\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/visual%20studio%20code/%E7%A0%94%E7%A9%B6%E7%94%9F/machine-learning/markdown/07%E5%91%A8%E6%8A%A5/code/sklearn/%E8%B0%B1%E5%8D%8F%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E9%9B%86%E7%BE%A4%E5%8C%96.ipynb#ch0000000?line=33'>34</a>\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mcomp.sys.ibm.pc.hardware\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcomp.sys.mac.hardware\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/visual%20studio%20code/%E7%A0%94%E7%A9%B6%E7%94%9F/machine-learning/markdown/07%E5%91%A8%E6%8A%A5/code/sklearn/%E8%B0%B1%E5%8D%8F%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E9%9B%86%E7%BE%A4%E5%8C%96.ipynb#ch0000000?line=34'>35</a>\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mcomp.windows.x\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmisc.forsale\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrec.autos\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/visual%20studio%20code/%E7%A0%94%E7%A9%B6%E7%94%9F/machine-learning/markdown/07%E5%91%A8%E6%8A%A5/code/sklearn/%E8%B0%B1%E5%8D%8F%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E9%9B%86%E7%BE%A4%E5%8C%96.ipynb#ch0000000?line=38'>39</a>\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mtalk.politics.guns\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtalk.politics.mideast\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/visual%20studio%20code/%E7%A0%94%E7%A9%B6%E7%94%9F/machine-learning/markdown/07%E5%91%A8%E6%8A%A5/code/sklearn/%E8%B0%B1%E5%8D%8F%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E9%9B%86%E7%BE%A4%E5%8C%96.ipynb#ch0000000?line=39'>40</a>\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mtalk.politics.misc\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtalk.religion.misc\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/visual%20studio%20code/%E7%A0%94%E7%A9%B6%E7%94%9F/machine-learning/markdown/07%E5%91%A8%E6%8A%A5/code/sklearn/%E8%B0%B1%E5%8D%8F%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E9%9B%86%E7%BE%A4%E5%8C%96.ipynb#ch0000000?line=40'>41</a>\u001b[0m newsgroups \u001b[39m=\u001b[39m fetch_20newsgroups(categories\u001b[39m=\u001b[39;49mcategories)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/visual%20studio%20code/%E7%A0%94%E7%A9%B6%E7%94%9F/machine-learning/markdown/07%E5%91%A8%E6%8A%A5/code/sklearn/%E8%B0%B1%E5%8D%8F%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E9%9B%86%E7%BE%A4%E5%8C%96.ipynb#ch0000000?line=41'>42</a>\u001b[0m y_true \u001b[39m=\u001b[39m newsgroups\u001b[39m.\u001b[39mtarget\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/visual%20studio%20code/%E7%A0%94%E7%A9%B6%E7%94%9F/machine-learning/markdown/07%E5%91%A8%E6%8A%A5/code/sklearn/%E8%B0%B1%E5%8D%8F%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E9%9B%86%E7%BE%A4%E5%8C%96.ipynb#ch0000000?line=43'>44</a>\u001b[0m vectorizer \u001b[39m=\u001b[39m NumberNormalizingVectorizer(stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m, min_df\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "File \u001b[1;32me:\\visual studio code\\研究生\\machine-learning\\markdown\\other\\jupyter\\lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:269\u001b[0m, in \u001b[0;36mfetch_20newsgroups\u001b[1;34m(data_home, subset, categories, shuffle, random_state, remove, download_if_missing, return_X_y)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39mif\u001b[39;00m download_if_missing:\n\u001b[0;32m    268\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mDownloading 20news dataset. This may take a few minutes.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 269\u001b[0m     cache \u001b[39m=\u001b[39m _download_20newsgroups(\n\u001b[0;32m    270\u001b[0m         target_dir\u001b[39m=\u001b[39;49mtwenty_home, cache_path\u001b[39m=\u001b[39;49mcache_path\n\u001b[0;32m    271\u001b[0m     )\n\u001b[0;32m    272\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m20Newsgroups dataset not found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\visual studio code\\研究生\\machine-learning\\markdown\\other\\jupyter\\lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:74\u001b[0m, in \u001b[0;36m_download_20newsgroups\u001b[1;34m(target_dir, cache_path)\u001b[0m\n\u001b[0;32m     71\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(target_dir)\n\u001b[0;32m     73\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mDownloading dataset from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (14 MB)\u001b[39m\u001b[39m\"\u001b[39m, ARCHIVE\u001b[39m.\u001b[39murl)\n\u001b[1;32m---> 74\u001b[0m archive_path \u001b[39m=\u001b[39m _fetch_remote(ARCHIVE, dirname\u001b[39m=\u001b[39;49mtarget_dir)\n\u001b[0;32m     76\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mDecompressing \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, archive_path)\n\u001b[0;32m     77\u001b[0m tarfile\u001b[39m.\u001b[39mopen(archive_path, \u001b[39m\"\u001b[39m\u001b[39mr:gz\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mextractall(path\u001b[39m=\u001b[39mtarget_dir)\n",
      "File \u001b[1;32me:\\visual studio code\\研究生\\machine-learning\\markdown\\other\\jupyter\\lib\\site-packages\\sklearn\\datasets\\_base.py:1511\u001b[0m, in \u001b[0;36m_fetch_remote\u001b[1;34m(remote, dirname)\u001b[0m\n\u001b[0;32m   1489\u001b[0m \u001b[39m\"\"\"Helper function to download a remote dataset into path\u001b[39;00m\n\u001b[0;32m   1490\u001b[0m \n\u001b[0;32m   1491\u001b[0m \u001b[39mFetch a dataset pointed by remote's url, save into path using remote's\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1507\u001b[0m \u001b[39m    Full path of the created file.\u001b[39;00m\n\u001b[0;32m   1508\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m file_path \u001b[39m=\u001b[39m remote\u001b[39m.\u001b[39mfilename \u001b[39mif\u001b[39;00m dirname \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m join(dirname, remote\u001b[39m.\u001b[39mfilename)\n\u001b[1;32m-> 1511\u001b[0m urlretrieve(remote\u001b[39m.\u001b[39;49murl, file_path)\n\u001b[0;32m   1512\u001b[0m checksum \u001b[39m=\u001b[39m _sha256(file_path)\n\u001b[0;32m   1513\u001b[0m \u001b[39mif\u001b[39;00m remote\u001b[39m.\u001b[39mchecksum \u001b[39m!=\u001b[39m checksum:\n",
      "File \u001b[1;32mD:\\language\\python\\lib\\urllib\\request.py:270\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    267\u001b[0m     reporthook(blocknum, bs, size)\n\u001b[0;32m    269\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 270\u001b[0m     block \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mread(bs)\n\u001b[0;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m block:\n\u001b[0;32m    272\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mD:\\language\\python\\lib\\http\\client.py:464\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    462\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    463\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 464\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[0;32m    465\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    466\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mD:\\language\\python\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mD:\\language\\python\\lib\\ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1270\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1271\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1272\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1273\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1274\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1275\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mD:\\language\\python\\lib\\ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1128\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import operator\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import SpectralCoclustering\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "def number_normalizer(tokens):\n",
    "    \"\"\" Map all numeric tokens to a placeholder.\n",
    "\n",
    "    For many applications, tokens that begin with a number are not directly\n",
    "    useful, but the fact that such a token exists can be relevant.  By applying\n",
    "    this form of dimensionality reduction, some methods may perform better.\n",
    "    \"\"\"\n",
    "    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)\n",
    "\n",
    "\n",
    "class NumberNormalizingVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super().build_tokenizer()\n",
    "        return lambda doc: list(number_normalizer(tokenize(doc)))\n",
    "\n",
    "\n",
    "# exclude 'comp.os.ms-windows.misc'\n",
    "categories = ['alt.atheism', 'comp.graphics',\n",
    "              'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
    "              'comp.windows.x', 'misc.forsale', 'rec.autos',\n",
    "              'rec.motorcycles', 'rec.sport.baseball',\n",
    "              'rec.sport.hockey', 'sci.crypt', 'sci.electronics',\n",
    "              'sci.med', 'sci.space', 'soc.religion.christian',\n",
    "              'talk.politics.guns', 'talk.politics.mideast',\n",
    "              'talk.politics.misc', 'talk.religion.misc']\n",
    "newsgroups = fetch_20newsgroups(categories=categories)\n",
    "y_true = newsgroups.target\n",
    "\n",
    "vectorizer = NumberNormalizingVectorizer(stop_words='english', min_df=5)\n",
    "cocluster = SpectralCoclustering(n_clusters=len(categories),\n",
    "                                 svd_method='arpack', random_state=0)\n",
    "kmeans = MiniBatchKMeans(n_clusters=len(categories), batch_size=20000,\n",
    "                         random_state=0)\n",
    "\n",
    "print(\"Vectorizing...\")\n",
    "X = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "print(\"Coclustering...\")\n",
    "start_time = time()\n",
    "cocluster.fit(X)\n",
    "y_cocluster = cocluster.row_labels_\n",
    "print(\"Done in {:.2f}s. V-measure: {:.4f}\".format(\n",
    "    time() - start_time,\n",
    "    v_measure_score(y_cocluster, y_true)))\n",
    "\n",
    "print(\"MiniBatchKMeans...\")\n",
    "start_time = time()\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "print(\"Done in {:.2f}s. V-measure: {:.4f}\".format(\n",
    "    time() - start_time,\n",
    "    v_measure_score(y_kmeans, y_true)))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "document_names = list(newsgroups.target_names[i] for i in newsgroups.target)\n",
    "\n",
    "\n",
    "def bicluster_ncut(i):\n",
    "    rows, cols = cocluster.get_indices(i)\n",
    "    if not (np.any(rows) and np.any(cols)):\n",
    "        import sys\n",
    "        return sys.float_info.max\n",
    "    row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]\n",
    "    col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]\n",
    "    # Note: the following is identical to X[rows[:, np.newaxis],\n",
    "    # cols].sum() but much faster in scipy <= 0.16\n",
    "    weight = X[rows][:, cols].sum()\n",
    "    cut = (X[row_complement][:, cols].sum() +\n",
    "           X[rows][:, col_complement].sum())\n",
    "    return cut / weight\n",
    "\n",
    "\n",
    "def most_common(d):\n",
    "    \"\"\"Items of a defaultdict(int) with the highest values.\n",
    "\n",
    "    Like Counter.most_common in Python >=2.7.\n",
    "    \"\"\"\n",
    "    return sorted(d.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "\n",
    "bicluster_ncuts = list(bicluster_ncut(i)\n",
    "                       for i in range(len(newsgroups.target_names)))\n",
    "best_idx = np.argsort(bicluster_ncuts)[:5]\n",
    "\n",
    "print()\n",
    "print(\"Best biclusters:\")\n",
    "print(\"----------------\")\n",
    "for idx, cluster in enumerate(best_idx):\n",
    "    n_rows, n_cols = cocluster.get_shape(cluster)\n",
    "    cluster_docs, cluster_words = cocluster.get_indices(cluster)\n",
    "    if not len(cluster_docs) or not len(cluster_words):\n",
    "        continue\n",
    "\n",
    "    # categories\n",
    "    counter = defaultdict(int)\n",
    "    for i in cluster_docs:\n",
    "        counter[document_names[i]] += 1\n",
    "    cat_string = \", \".join(\"{:.0f}% {}\".format(float(c) / n_rows * 100, name)\n",
    "                           for name, c in most_common(counter)[:3])\n",
    "\n",
    "    # words\n",
    "    out_of_cluster_docs = cocluster.row_labels_ != cluster\n",
    "    out_of_cluster_docs = np.where(out_of_cluster_docs)[0]\n",
    "    word_col = X[:, cluster_words]\n",
    "    word_scores = np.array(word_col[cluster_docs, :].sum(axis=0) -\n",
    "                           word_col[out_of_cluster_docs, :].sum(axis=0))\n",
    "    word_scores = word_scores.ravel()\n",
    "    important_words = list(feature_names[cluster_words[i]]\n",
    "                           for i in word_scores.argsort()[:-11:-1])\n",
    "\n",
    "    print(\"bicluster {} : {} documents, {} words\".format(\n",
    "        idx, n_rows, n_cols))\n",
    "    print(\"categories   : {}\".format(cat_string))\n",
    "    print(\"words        : {}\\n\".format(', '.join(important_words)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('jupyter': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "141c5e0ba87e97013844de5b21d12d023199bf67a08052ba51b9f2de1a481a39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
